{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb26228-5fdb-48bc-9f27-06961e5c6432",
   "metadata": {},
   "source": [
    "## Weekly Stock Pick\n",
    "This should be a self contained notebook which can be run top to bottom each week. The output should be 5-10 companies which can be logged and their performance recorded as a paper test. The notebook should be run on a weekend, the idea being that if productionised, it would be run friday night before market close.\n",
    "The order of operations should be:\n",
    "1) Scrape historic data, save to a holding directory\n",
    "2) Resample the data to be weekly such that the entire current week is considered as the most up to date data point\n",
    "3) Attach relevant technical indicators and resave in another directory. This should be a modular process in case I want to add technical indicators in the future. Currently the RSI, MFI, ULT and my personal volume ranker are used.\n",
    "4) For the most recent month, rank order all companies by the indicator scores, take 3 from each and combine them for unique codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5340645-e8f6-40cc-ad7b-ed8dfc1099db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea5317-587b-44c4-856b-e5f85f555da1",
   "metadata": {},
   "source": [
    "### Step 1 -> Tickers\n",
    "The ticker symbols for consideration should be all companies listed on the NYSE and nasdaq exchanges. The web link contains a download button for these tickers. The file should be renamed and placed in the working directory. They don't tend to change much so this should'nt need to be redone very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c203ae-1b03-4751-9e5d-cbdd9a01c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nasdaq.com/market-activity/stocks/screener\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tickers.csv')\n",
    "\n",
    "# Extract the 'Symbol' column into a list\n",
    "ticker_symbols = df['Symbol'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cde0b-478a-46d6-97d4-2aee26ebb216",
   "metadata": {},
   "source": [
    "### Step 2 -> Scraping\n",
    "The list of tickers is north of 7,000 so we have chunked it up to give some visibile of the progress for the download but also for fault tolerance as some of the tickers are obscure and can throw errors. What we should get out of this is a directory full of files containing the typical daily market data including opens, closes, highs, lows and volumes. Important note for this code is that anything with an odd symbol is not being handled. There will be many symbols with ^ or / inside the string which don't work, some may not have any data, some may be delisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1065aea-1e4d-4963-8a7f-a1322683b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Functions\n",
    "\"\"\"\n",
    "# Function to fetch and save data for a list of tickers\n",
    "def fetch_and_save_data(tickers):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365*10)\n",
    "    \n",
    "    try:\n",
    "        #print(f\"Fetching data for tickers: {tickers}\")\n",
    "        data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            ticker_data = data[ticker]\n",
    "            if not ticker_data.empty:\n",
    "                file_path = os.path.join(data_dir, f\"{ticker}.csv\")\n",
    "                ticker_data.to_csv(file_path)\n",
    "                print(f\"Data for {ticker} saved successfully.\")\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for tickers: {e}\")\n",
    "\n",
    "# Function to split the list into chunks of specified size\n",
    "def chunk_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "\"\"\"\n",
    "    Driver code\n",
    "\"\"\"\n",
    "\n",
    "# Directory to save the data\n",
    "data_dir = \"ticker_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Process tickers in chunks of 500\n",
    "chunk_size = 500\n",
    "for ticker_chunk in chunk_list(ticker_symbols, chunk_size):\n",
    "    fetch_and_save_data(ticker_chunk)\n",
    "\n",
    "print(\"Data fetching completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae7ffd-e10a-4087-a7dc-07a4cbcf6c12",
   "metadata": {},
   "source": [
    "### Step 3 -> Resampling\n",
    "The data we have scraped is daily. There are a few issues with this. One issue is that there is no guarantee that any ticker has the same days in it as any other. Some may have data which ends sooner or starts later. To get around most of the issues, we can use a merge function on the date index to combine them and have nulls for the missings, which is fine for our purposes. Additionally, we dont want to process daily data as its less useful to weekly resampling gives us appropriate data for the sort of technical indicators we are playing with. Lastly, we want the resample to cap off at the most recent Friday so final date in the output should be the last Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c174033a-ae5c-464b-998c-bf0efddb2899",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         ticker_df\u001b[38;5;241m.\u001b[39mto_csv(output_filepath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mprocess_ticker_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mticker_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36mprocess_ticker_data\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     11\u001b[0m         ticker_symbols\u001b[38;5;241m.\u001b[39mappend(ticker)\n\u001b[0;32m     12\u001b[0m         filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file)\n\u001b[1;32m---> 13\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         dataframes[ticker] \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2: Create New Column Names\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_ticker_data(directory: str):\n",
    "    # Step 1: Load all CSV files into DataFrames\n",
    "    dataframes = {}\n",
    "    ticker_symbols = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            ticker = file.split('.csv')[0]\n",
    "            ticker_symbols.append(ticker)\n",
    "            filepath = os.path.join(directory, file)\n",
    "            df = pd.read_csv(filepath, parse_dates=['Date'])\n",
    "            dataframes[ticker] = df\n",
    "    \n",
    "    # Step 2: Create New Column Names\n",
    "    for ticker, df in dataframes.items():\n",
    "        df.rename(columns={\n",
    "            'Open': f'{ticker}_Open',\n",
    "            'High': f'{ticker}_High',\n",
    "            'Low': f'{ticker}_Low',\n",
    "            'Close': f'{ticker}_Close',\n",
    "            'Adj Close': f'{ticker}_Adj_Close',\n",
    "            'Volume': f'{ticker}_Volume'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # Step 3: Merge DataFrames on the 'Date' Column\n",
    "    merged_df = None\n",
    "    for ticker, df in dataframes.items():\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='Date', how='outer')\n",
    "    \n",
    "    # Step 4: Resample the DataFrame Weekly (ending on Fridays)\n",
    "    merged_df.set_index('Date', inplace=True)\n",
    "    weekly_df = merged_df.resample('W-FRI').last().reset_index()\n",
    "    \n",
    "    # Step 5: Split and Save DataFrames\n",
    "    for ticker in ticker_symbols:\n",
    "        ticker_columns = [col for col in weekly_df.columns if (col.split('_')[0] == ticker)]\n",
    "        ticker_columns.insert(0, 'Date')  # Ensure 'Date' column is included\n",
    "        ticker_df = weekly_df[ticker_columns]\n",
    "        output_filepath = os.path.join('weekly_data', f'{ticker}.csv')\n",
    "        ticker_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "# Example usage:\n",
    "process_ticker_data('ticker_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f61bf-7a64-4e51-b455-f0dda0e8bab6",
   "metadata": {},
   "source": [
    "### Step 4 -> Technical Indicators\n",
    "We need standardised measures of stocks which can be rank ordered in a way where the ranking of the indicator relative to the wider market gives us value. Good example is RSI where the most overbought companies in the Nasdaq and NYSE have significant positive sentiment behind them when the data was taken. Backtests verify that this sentiment tends to continue more often than it reverses so using it as a leading indicator of stock price growth works over the long term. MFI is a normalised version of my original indicator which just multiplies volume into adj close and rank orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b1c5b-47bf-4b68-a971-d2022e4be3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Functions\n",
    "\"\"\"\n",
    "\n",
    "def calculate_rsi(df, ticker, period=14):\n",
    "    column = f'{ticker}_Close'\n",
    "    delta = df[column].diff()\n",
    "    gain = delta.clip(lower=0).rolling(window=period, min_periods=1).mean()\n",
    "    loss = -delta.clip(upper=0).rolling(window=period, min_periods=1).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_mfi(df, ticker, period=14):\n",
    "    typical_price = (df[f'{ticker}_High'] + df[f'{ticker}_Low'] + df[f'{ticker}_Close']) / 3\n",
    "    money_fLow = typical_price * df[f'{ticker}_Volume']\n",
    "    positive_fLow = money_fLow.where(typical_price > typical_price.shift(1), 0).rolling(window=period, min_periods=1).sum()\n",
    "    negative_fLow = money_fLow.where(typical_price < typical_price.shift(1), 0).rolling(window=period, min_periods=1).sum()\n",
    "    mfi = 100 - (100 / (1 + positive_fLow / negative_fLow))\n",
    "    return mfi\n",
    "\n",
    "def calculate_ultimate_oscillator(df, ticker, short_period=7, mid_period=14, long_period=28):\n",
    "    # Ensure the DataFrame is sorted by date\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Calculate Buying Pressure (BP) and True Range (TR)\n",
    "    df['Prior Close'] = df[f'{ticker}_Close'].shift(1)\n",
    "    df['BP'] = df[f'{ticker}_Close'] - df[[f'{ticker}_Low', 'Prior Close']].min(axis=1)\n",
    "    df['TR'] = df[[f'{ticker}_High', 'Prior Close']].max(axis=1) - df[[f'{ticker}_Low', 'Prior Close']].min(axis=1)\n",
    "    \n",
    "    # Calculate average BP and TR for each period\n",
    "    df['Avg7_BP'] = df['BP'].rolling(window=short_period).sum()\n",
    "    df['Avg7_TR'] = df['TR'].rolling(window=short_period).sum()\n",
    "    df['Avg14_BP'] = df['BP'].rolling(window=mid_period).sum()\n",
    "    df['Avg14_TR'] = df['TR'].rolling(window=mid_period).sum()\n",
    "    df['Avg28_BP'] = df['BP'].rolling(window=long_period).sum()\n",
    "    df['Avg28_TR'] = df['TR'].rolling(window=long_period).sum()\n",
    "    \n",
    "    # Calculate raw UO components\n",
    "    df['R1'] = df['Avg7_BP'] / df['Avg7_TR']\n",
    "    df['R2'] = df['Avg14_BP'] / df['Avg14_TR']\n",
    "    df['R3'] = df['Avg28_BP'] / df['Avg28_TR']\n",
    "    \n",
    "    # Calculate Ultimate Oscillator\n",
    "    df['Ultimate Oscillator'] = 100 * (4 * df['R1'] + 2 * df['R2'] + df['R3']) / (4 + 2 + 1)\n",
    "    \n",
    "    return df['Ultimate Oscillator']\n",
    "\n",
    "def calculate_obv(df, ticker):\n",
    "    # Calculate daily returns\n",
    "    df['Daily Return'] = df[f'{ticker}_Close'].diff()\n",
    "    \n",
    "    # Calculate the direction of the volume flow\n",
    "    df['Direction'] = 0\n",
    "    df.loc[df['Daily Return'] > 0, 'Direction'] = 1\n",
    "    df.loc[df['Daily Return'] < 0, 'Direction'] = -1\n",
    "    \n",
    "    # Calculate OBV\n",
    "    df['OBV'] = (df[f'{ticker}_Volume'] * df['Direction']).cumsum()\n",
    "    \n",
    "    \n",
    "    return df['OBV']\n",
    "    \n",
    "def calculate_close_open_avg_volume(df, ticker):\n",
    "    # Calculate the average of 'Close' and 'Open' prices\n",
    "    df['Close_Open_Avg'] = (df[f'{ticker}_Close'] + df[f'{ticker}_Open']) / 2\n",
    "    \n",
    "    # Multiply the average with 'Volume'\n",
    "    df['Close_Open_Avg_Volume'] = df['Close_Open_Avg'] * df[f'{ticker}_Volume']\n",
    "    \n",
    "    return df['Close_Open_Avg_Volume']\n",
    "    \n",
    "def add_technical_indicators():\n",
    "    weekly_data_dir = 'weekly_data'\n",
    "    ta_data_dir = 'ta_data'\n",
    "    \n",
    "    if not os.path.exists(ta_data_dir):\n",
    "        os.makedirs(ta_data_dir)\n",
    "    \n",
    "    # List all CSV files in the weekly_data directory\n",
    "    ticker_files = [f for f in os.listdir(weekly_data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    for file in ticker_files:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(weekly_data_dir, file))\n",
    "        \n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Ensure the date column is in datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Sort the dataframe by date\n",
    "        df.sort_values(by='Date', inplace=True)\n",
    "\n",
    "        ticker = file.split('.')[0]\n",
    "        print(ticker)\n",
    "        \n",
    "        # Calculate technical indicators\n",
    "        df['RSI'] = calculate_rsi(df, ticker)\n",
    "        df['MFI'] = calculate_mfi(df, ticker)\n",
    "        df['ULTOSC'] = calculate_ultimate_oscillator(df, ticker)\n",
    "        df['OBV'] = calculate_obv(df, ticker)\n",
    "        df['MON'] = calculate_close_open_avg_volume(df, ticker)\n",
    "        \n",
    "        \n",
    "        # Remove any rows with NaN values introduced by the indicators\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Only take the columns we need for writing\n",
    "        final_df = df[['Date', 'RSI', 'MFI', 'ULTOSC', 'OBV', 'MON', f'{ticker}_Adj_Close']]\n",
    "        \n",
    "        final_df.rename(columns={col: 'Close' if col.endswith('Adj_Close') else col for col in final_df.columns}, inplace=True)\n",
    "        \n",
    "        # Save the updated DataFrame to the ta_data directory\n",
    "        final_df.to_csv(os.path.join(ta_data_dir, file), index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Driver code\n",
    "\"\"\"\n",
    "\n",
    "# Call the function to execute the task\n",
    "add_technical_indicators()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2040bc6-3e80-4341-a61b-0e9d70f1b7ca",
   "metadata": {},
   "source": [
    "### Step 5 -> Rank Ordering\n",
    "This step loads in the previously saved data with the technical indicators attached. Now that we have this data across a standard date dimension and resampled at the weekly level, the highest number across each date can be observed as the hot stock of the day. When this is done, many small and tiny cap companies pop up as the small volume produces false signals when small amounts of money are used to buy or sell. We introduce an arbitrary minimum spend for the past week of 1 million dollars to remove these. What is left is the companies with the most positive sentiment against them which we then output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca9f54-8047-4ef2-8f32-9aa8033b5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def load_ta_data():\n",
    "    ta_data_dir = 'ta_data'\n",
    "    \n",
    "    # List all CSV files in the ta_data directory\n",
    "    ticker_files = [f for f in os.listdir(ta_data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in ticker_files:\n",
    "        # Extract the ticker symbol from the file name (assuming it's the file name without extension)\n",
    "        ticker_symbol = os.path.splitext(file)[0].split('_')[0]\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(ta_data_dir, file))\n",
    "        \n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Add a column for the ticker symbol\n",
    "        df['ticker'] = ticker_symbol\n",
    "        \n",
    "        # Remove rows where 'MON' column is less than 1 million\n",
    "        df = df[df['MON'] >= 1_000_000]   \n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(dataframes)\n",
    "    \n",
    "    # Ensure the date column is in datetime format\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "    \n",
    "    # Set the date column as the index\n",
    "    combined_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "combined_df = load_ta_data()\n",
    "#print(combined_df)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cbe87de-5dec-440b-930f-9f5d965aa974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcarp\\AppData\\Local\\Temp\\ipykernel_7684\\2637258726.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date                        Top_RSI  \\\n",
      "0    2014-12-26  [ROST, BBWI, WELL, EQR, ORLY]   \n",
      "1    2015-01-02     [WELL, SKT, ROST, HA, LOW]   \n",
      "2    2015-01-09    [WELL, SKT, SPG, ROST, EQR]   \n",
      "3    2015-01-16    [WELL, SKT, SHW, SPG, BERY]   \n",
      "4    2015-01-23       [LOW, SPG, KR, SHW, UAL]   \n",
      "..          ...                            ...   \n",
      "490  2024-05-17    [PPC, VITL, VIRT, AEM, HBM]   \n",
      "491  2024-05-24   [VITL, LPG, FIP, PRMW, VIRT]   \n",
      "492  2024-05-31      [VITL, LPG, KGC, AA, FSM]   \n",
      "493  2024-06-07   [VITL, AGX, PRMW, AZN, NVAX]   \n",
      "494  2024-06-14     [EDR, VITL, AGX, HEI, AZN]   \n",
      "\n",
      "                            Top_MFI                      Top_ULTOSC  \\\n",
      "0      [KEX, EQNR, GOLD, HAL, FSLR]      [TPST, MAC, FTNT, LOW, HE]   \n",
      "1         [RES, BBVA, AY, PDS, KEX]       [TPST, MAC, HE, KMX, PNW]   \n",
      "2       [TAL, ABUS, BBVA, RES, PDS]       [TPST, HE, TNK, SUI, ELS]   \n",
      "3       [RYAM, WDS, RES, ASPS, OBE]    [TPST, TCRT, WELL, RMD, ESS]   \n",
      "4      [RYAM, SRV, ASPS, VSTM, SXC]  [LULU, TPST, WELL, SBRA, FTNT]   \n",
      "..                              ...                             ...   \n",
      "490    [COUR, ZS, ACAD, VRSN, FWRD]     [ZETA, SFM, PHG, VITL, GSL]   \n",
      "491    [COUR, ZS, FWRD, SIRI, GNLX]    [AIRC, SFM, ZETA, GSL, VLTO]   \n",
      "492  [COUR, RELY, XENE, FRSH, REYN]      [GSL, CRAI, PEG, HCP, SNX]   \n",
      "493  [COUR, BCDA, PRST, BEAM, XENE]   [WKME, HCP, SQSP, ADMA, TMUS]   \n",
      "494    [COUR, FLFV, CNSP, CP, FIVE]  [WKME, VRSK, SQSP, COST, AIRC]   \n",
      "\n",
      "                            Top_OBV                          Top_MON  \n",
      "0     [AAPL, NVDA, BAC, TSLA, SIRI]     [AAPL, GILD, XOM, TSLA, BAC]  \n",
      "1        [AAPL, NVDA, BAC, KMI, KO]      [AAPL, GILD, TSLA, GE, XOM]  \n",
      "2       [AAPL, NVDA, BAC, KO, SIRI]     [AAPL, GILD, BAC, XOM, SBUX]  \n",
      "3       [AAPL, NVDA, SIRI, KO, KMI]       [AAPL, BAC, JPM, XOM, JNJ]  \n",
      "4       [AAPL, NVDA, BAC, SIRI, KO]       [AAPL, UPS, SBUX, BAC, GE]  \n",
      "..                              ...                              ...  \n",
      "490  [NVDA, TSLA, AAPL, PLTR, OCGN]  [NVDA, TSLA, AAPL, SMCI, GOOGL]  \n",
      "491   [NVDA, TSLA, AAPL, PLTR, GME]   [NVDA, TSLA, AAPL, SMCI, WDAY]  \n",
      "492   [NVDA, TSLA, AAPL, PLTR, GME]  [NVDA, AAPL, TSLA, AVGO, GOOGL]  \n",
      "493   [NVDA, TSLA, AAPL, PLTR, GME]   [NVDA, AAPL, TSLA, GME, GOOGL]  \n",
      "494   [NVDA, TSLA, AAPL, PLTR, GME]   [NVDA, TSLA, AAPL, AVGO, ADBE]  \n",
      "\n",
      "[495 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing the CSV files\n",
    "directory = 'ta_data'\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all CSV files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        ticker_symbol = filename.split('.')[0]\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['Ticker'] = ticker_symbol\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df.loc[merged_df['MON'] < 10000000, :] = None\n",
    "# Initialize an empty list to store the summary data\n",
    "summary_data = []\n",
    "\n",
    "# Group by 'Date' and find the top 5 ticker symbols for each specified column\n",
    "for date, group in merged_df.groupby('Date'):\n",
    "    top_rsi = group.nlargest(5, 'RSI')[['Ticker', 'RSI']]\n",
    "    top_mfi = group.nsmallest(5, 'MFI')[['Ticker', 'MFI']]\n",
    "    top_ultosc = group.nlargest(5, 'ULTOSC')[['Ticker', 'ULTOSC']]\n",
    "    top_obv = group.nlargest(5, 'OBV')[['Ticker', 'OBV']]\n",
    "    top_mon = group.nlargest(5, 'MON')[['Ticker', 'MON']]\n",
    "    \n",
    "    # Create a summary dictionary for the current date\n",
    "    summary = {\n",
    "        'Date': date,\n",
    "        'Top_RSI': top_rsi['Ticker'].tolist(),\n",
    "        'Top_MFI': top_mfi['Ticker'].tolist(),\n",
    "        'Top_ULTOSC': top_ultosc['Ticker'].tolist(),\n",
    "        'Top_OBV': top_obv['Ticker'].tolist(),\n",
    "        'Top_MON': top_mon['Ticker'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Append the summary to the list\n",
    "    summary_data.append(summary)\n",
    "\n",
    "# Convert the summary data list to a dataframe\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766be287-4e02-4b75-945c-b9b7146b5ccd",
   "metadata": {},
   "source": [
    "### Step 6 -> Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee00eac5-8d28-460a-9edf-ab432e44929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date                                   Top_RSI_Pct_Diff  \\\n",
      "0    2014-12-26  {'BBWI': -9.032164691383137, 'EQR': -1.8840596...   \n",
      "1    2015-01-02                       {'SKT': -0.3261868490705755}   \n",
      "2    2015-01-09  {'EQR': -0.24461878739424625, 'ROST': 0.342594...   \n",
      "3    2015-01-16  {'BERY': -0.15754349091625652, 'SHW': 4.564814...   \n",
      "4    2015-01-23  {'KR': 1.3669211790297453, 'LOW': -0.151445084...   \n",
      "..          ...                                                ...   \n",
      "490  2024-05-17  {'AEM': 5.166044302393513, 'HBM': 4.8984450778...   \n",
      "491  2024-05-24  {'FIP': 5.851759203561668, 'LPG': 1.2972003637...   \n",
      "492  2024-05-31  {'AA': 1.2645925857757145, 'FSM': 4.1095850145...   \n",
      "493  2024-06-07  {'AGX': 0.15598499090736517, 'AZN': -0.6620884...   \n",
      "494  2024-06-14  {'AGX': 8.098543739397247, 'AZN': 2.5634453807...   \n",
      "\n",
      "                                      Top_MFI_Pct_Diff  \\\n",
      "0    {'EQNR': 1.7082778994878423, 'FSLR': -5.286130...   \n",
      "1    {'AY': 17.684894189294397, 'BBVA': 3.288849187...   \n",
      "2    {'ABUS': -24.067802253860982, 'RES': -0.225062...   \n",
      "3    {'ASPS': -13.473058987323538, 'OBE': -2.816901...   \n",
      "4    {'ASPS': -4.290656704102858, 'SRV': -16.561516...   \n",
      "..                                                 ...   \n",
      "490  {'ACAD': -10.557180748419025, 'COUR': -8.11654...   \n",
      "491  {'COUR': -3.8505113876058594, 'FWRD': -7.51295...   \n",
      "492  {'COUR': -9.422847296406257, 'FRSH': -2.932555...   \n",
      "493  {'BEAM': -3.249395276763478, 'COUR': -1.170353...   \n",
      "494  {'CNSP': -1.7021276595744705, 'COUR': -6.05263...   \n",
      "\n",
      "                                   Top_ULTOSC_Pct_Diff  \\\n",
      "0    {'FTNT': 38.93804992316097, 'HE': -3.723885911...   \n",
      "1                                                   {}   \n",
      "2    {'ELS': 0.8863169135946647, 'HE': -0.767629063...   \n",
      "3    {'ESS': 4.882339646425238, 'RMD': 1.7316625881...   \n",
      "4    {'FTNT': -0.48278417640426063, 'LULU': 7.45352...   \n",
      "..                                                 ...   \n",
      "490  {'GSL': 4.494386097488756, 'PHG': 0.2237116578...   \n",
      "491  {'AIRC': 0.31088795680263015, 'GSL': 10.794047...   \n",
      "492  {'CRAI': 40.41115511598428, 'GSL': 6.905555845...   \n",
      "493  {'ADMA': -0.10460490003029177, 'HCP': 0.569039...   \n",
      "494  {'AIRC': -0.051614084551410144, 'COST': 4.4067...   \n",
      "\n",
      "                                      Top_OBV_Pct_Diff  \\\n",
      "0    {'AAPL': -1.7962273602035728, 'BAC': -4.420173...   \n",
      "1                                                   {}   \n",
      "2    {'AAPL': -4.0880734335082325, 'BAC': -0.444935...   \n",
      "3    {'AAPL': 2.451276512024192, 'KMI': -2.33590808...   \n",
      "4    {'AAPL': -5.3745185676636, 'BAC': -9.422865917...   \n",
      "..                                                 ...   \n",
      "490  {'AAPL': -0.17995518718934012, 'NVDA': 1.22649...   \n",
      "491  {'AAPL': 3.72575359285916, 'GME': 27.205041518...   \n",
      "492  {'AAPL': 0.057934699099582154, 'GME': -14.4529...   \n",
      "493  {'AAPL': 1.1948648928893135, 'GME': 21.7894704...   \n",
      "494  {'AAPL': 2.4135237397391096, 'GME': 21.9533278...   \n",
      "\n",
      "                                      Top_MON_Pct_Diff  \n",
      "0    {'AAPL': -1.7962273602035728, 'BAC': -4.420173...  \n",
      "1                                                   {}  \n",
      "2    {'AAPL': -4.0880734335082325, 'BAC': -0.444935...  \n",
      "3    {'AAPL': 2.451276512024192, 'BAC': -5.13966021...  \n",
      "4    {'AAPL': -5.3745185676636, 'BAC': -9.422865917...  \n",
      "..                                                 ...  \n",
      "490  {'AAPL': -0.17995518718934012, 'GOOGL': 0.8430...  \n",
      "491  {'AAPL': 3.72575359285916, 'NVDA': 2.893920617...  \n",
      "492  {'AAPL': 0.057934699099582154, 'AVGO': 0.89944...  \n",
      "493  {'AAPL': 1.1948648928893135, 'GME': 21.7894704...  \n",
      "494  {'AAPL': 2.4135237397391096, 'ADBE': 4.6474463...  \n",
      "\n",
      "[495 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the dataframe by 'Ticker' and 'Date'\n",
    "merged_df.sort_values(by=['Ticker', 'Date'], inplace=True)\n",
    "\n",
    "# Calculate the percentage difference in 'Close' column for each ticker\n",
    "merged_df['Close_Pct_Diff'] = merged_df.groupby('Ticker')['Close'].pct_change() * 100\n",
    "merged_df['Close_Pct_Diff'] = merged_df['Close_Pct_Diff'].shift(1)\n",
    "\n",
    "# Initialize an empty list to store the summary data with percentage differences\n",
    "summary_pct_diff_data = []\n",
    "\n",
    "# Loop through each date and the corresponding top tickers from the previous summary\n",
    "for index, row in summary_df.iterrows():\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Extract the top tickers for each indicator on the current date\n",
    "    top_rsi_tickers = row['Top_RSI']\n",
    "    top_mfi_tickers = row['Top_MFI']\n",
    "    top_ultosc_tickers = row['Top_ULTOSC']\n",
    "    top_obv_tickers = row['Top_OBV']\n",
    "    top_mon_tickers = row['Top_MON']\n",
    "    \n",
    "    # Find the percentage differences for the top tickers\n",
    "    top_rsi_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_rsi_tickers))][['Ticker', 'Close_Pct_Diff']].dropna()\n",
    "    top_mfi_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_mfi_tickers))][['Ticker', 'Close_Pct_Diff']].dropna()\n",
    "    top_ultosc_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_ultosc_tickers))][['Ticker', 'Close_Pct_Diff']].dropna()\n",
    "    top_obv_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_obv_tickers))][['Ticker', 'Close_Pct_Diff']].dropna()\n",
    "    top_mon_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_mon_tickers))][['Ticker', 'Close_Pct_Diff']].dropna()\n",
    "    \n",
    "    # Create a summary dictionary for the current date with percentage differences\n",
    "    summary_pct_diff = {\n",
    "        'Date': date,\n",
    "        'Top_RSI_Pct_Diff': top_rsi_pct_diff.set_index('Ticker')['Close_Pct_Diff'].to_dict(),\n",
    "        'Top_MFI_Pct_Diff': top_mfi_pct_diff.set_index('Ticker')['Close_Pct_Diff'].to_dict(),\n",
    "        'Top_ULTOSC_Pct_Diff': top_ultosc_pct_diff.set_index('Ticker')['Close_Pct_Diff'].to_dict(),\n",
    "        'Top_OBV_Pct_Diff': top_obv_pct_diff.set_index('Ticker')['Close_Pct_Diff'].to_dict(),\n",
    "        'Top_MON_Pct_Diff': top_mon_pct_diff.set_index('Ticker')['Close_Pct_Diff'].to_dict()\n",
    "    }\n",
    "    \n",
    "    # Append the summary with percentage differences to the list\n",
    "    summary_pct_diff_data.append(summary_pct_diff)\n",
    "\n",
    "# Convert the summary data with percentage differences list to a dataframe\n",
    "summary_pct_diff_df = pd.DataFrame(summary_pct_diff_data)\n",
    "\n",
    "print(summary_pct_diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b2ea37b-4024-4768-bc35-981296e27fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Avg_RSI_Pct_Diff  Avg_MFI_Pct_Diff  Avg_ULTOSC_Pct_Diff  \\\n",
      "0    2014-12-26         -3.914299          5.811881            11.567331   \n",
      "1    2015-01-02         -0.326187          8.416426                  NaN   \n",
      "2    2015-01-09          0.172725         -9.013236             0.541593   \n",
      "3    2015-01-16          3.517133         -6.280910             4.943277   \n",
      "4    2015-01-23          0.755258         -7.864028             3.343370   \n",
      "..          ...               ...               ...                  ...   \n",
      "490  2024-05-17          8.332611         -9.492014            12.026822   \n",
      "491  2024-05-24          3.261113         -2.398849             5.456885   \n",
      "492  2024-05-31          4.770342         -4.173599            10.433224   \n",
      "493  2024-06-07         -0.367699         -2.330124             1.735437   \n",
      "494  2024-06-14          2.447350         -5.375610             1.744256   \n",
      "\n",
      "     Avg_OBV_Pct_Diff  Avg_MON_Pct_Diff  \n",
      "0          -29.327979        -13.246538  \n",
      "1                 NaN               NaN  \n",
      "2           -1.792479         -0.844628  \n",
      "3            0.544061         -1.622742  \n",
      "4           -3.171821         -2.992399  \n",
      "..                ...               ...  \n",
      "490         -4.846552         -0.622393  \n",
      "491          8.958410          5.571112  \n",
      "492         -0.342179          3.296090  \n",
      "493          5.699575          4.777195  \n",
      "494          8.362887          4.573588  \n",
      "\n",
      "[495 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the summary data with average percentage changes\n",
    "summary_avg_pct_diff_data = []\n",
    "\n",
    "# Loop through each date and the corresponding top tickers from the previous summary\n",
    "for index, row in summary_df.iterrows():\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Extract the top tickers for each indicator on the current date\n",
    "    top_rsi_tickers = row['Top_RSI']\n",
    "    top_mfi_tickers = row['Top_MFI']\n",
    "    top_ultosc_tickers = row['Top_ULTOSC']\n",
    "    top_obv_tickers = row['Top_OBV']\n",
    "    top_mon_tickers = row['Top_MON']\n",
    "    \n",
    "    # Find the percentage differences for the top tickers and calculate the average\n",
    "    top_rsi_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_rsi_tickers))]['Close_Pct_Diff'].dropna().mean()\n",
    "    top_mfi_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_mfi_tickers))]['Close_Pct_Diff'].dropna().mean()\n",
    "    top_ultosc_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_ultosc_tickers))]['Close_Pct_Diff'].dropna().mean()\n",
    "    top_obv_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_obv_tickers))]['Close_Pct_Diff'].dropna().mean()\n",
    "    top_mon_pct_diff = merged_df[(merged_df['Date'] == date) & (merged_df['Ticker'].isin(top_mon_tickers))]['Close_Pct_Diff'].dropna().mean()\n",
    "    \n",
    "    # Create a summary dictionary for the current date with average percentage differences\n",
    "    summary_avg_pct_diff = {\n",
    "        'Date': date,\n",
    "        'Avg_RSI_Pct_Diff': top_rsi_pct_diff,\n",
    "        'Avg_MFI_Pct_Diff': top_mfi_pct_diff,\n",
    "        'Avg_ULTOSC_Pct_Diff': top_ultosc_pct_diff,\n",
    "        'Avg_OBV_Pct_Diff': top_obv_pct_diff,\n",
    "        'Avg_MON_Pct_Diff': top_mon_pct_diff\n",
    "    }\n",
    "    \n",
    "    # Append the summary with average percentage differences to the list\n",
    "    summary_avg_pct_diff_data.append(summary_avg_pct_diff)\n",
    "\n",
    "# Convert the summary data with average percentage differences list to a dataframe\n",
    "summary_avg_pct_diff_df = pd.DataFrame(summary_avg_pct_diff_data)\n",
    "print(summary_avg_pct_diff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73b0054e-8516-467f-a100-d21819c749ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.728679251335436"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_avg_pct_diff_df['Avg_ULTOSC_Pct_Diff'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d0437-26ce-4abc-9e54-326993333bbf",
   "metadata": {},
   "source": [
    "## Purge the Folders\n",
    "Can't push this much data to source control so once we have our output file, we remove the contents of the three folders we've been holding data in. Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aed4e17-debf-45b3-9bb2-820e07980c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def purge_directories(directories):\n",
    "    for directory in directories:\n",
    "        if os.path.exists(directory):\n",
    "            for filename in os.listdir(directory):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Directories to be purged\n",
    "directories = ['ta_data', 'ticker_data', 'weekly_data']\n",
    "\n",
    "# Purge the directories\n",
    "purge_directories(directories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e348a8-d3bb-405d-990b-5a7ce90020ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
